
# Bigram LLM

PyTorch implementation of Bigram Large Language Models, from scratch. The model trains on a piece of text and generates new text similar to it.
Trainable parameters : 25.37 million
Vocabulary size : 104



## Attention Mechanism

![App Screenshot](https://media.geeksforgeeks.org/wp-content/uploads/20240110170625/Scaled-Dot-Product-and-Multi-Head-Attentions.webp)


## Web Application
<img width="1440" alt="first" src="https://github.com/Aditya-S09/LLM/assets/78774850/0fbc9663-f33a-41f4-8253-1963f40d6bb4">
<img width="1440" alt="second" src="https://github.com/Aditya-S09/LLM/assets/78774850/166f2a8f-1185-48b0-a664-e801b64f4fbf">




## References

 - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

 ![App Screenshot](https://miro.medium.com/v2/resize:fit:1000/1*ekIrztJ8io3kDBLMxnVqDA.png)



